{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174f71ee",
   "metadata": {},
   "source": [
    "# Introduction to Backpropagation and Neural Networks with AutoGrad\n",
    "Following the video and code of Andrej Karpathy\n",
    "https://www.youtube.com/watch?v=VMj-3S1tku0&t=1072s&pp=ugMICgJlcxABGAHKBRJtaWNyb2dyYWQga2FycGF0aHk%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b075ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2636d2",
   "metadata": {},
   "source": [
    "Lets start by defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df3e1a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We can, of course, evaluate the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ee98c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3498362",
   "metadata": {},
   "source": [
    "Lets plot this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc59a19",
   "metadata": {},
   "source": [
    "What is derivative meassuring?\n",
    "\n",
    "df/dx = lim h->0 (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478daaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "x = 3.0\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df46592",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aded0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52650ae9",
   "metadata": {},
   "source": [
    "Solving analytically we find dy/dx = 6*x-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "6 * x - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a11dc",
   "metadata": {},
   "source": [
    "The derivative in the point is the slope, or instaneous increment of the function when the argument incresase:\n",
    "- since derivative is positive the funcion is increasing in that point\n",
    "- the increase is proportional to 14 times the increment in the function parameter\n",
    "\n",
    "Lets try a different value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05739504",
   "metadata": {},
   "source": [
    "Now the function is also increasing, but now slower\n",
    "\n",
    "Lets check another value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -2\n",
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7862d",
   "metadata": {},
   "source": [
    "Note than in x=2 the function is decreasing, faster ...\n",
    "\n",
    "Lets see another point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2/3\n",
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e97b50",
   "metadata": {},
   "source": [
    "It is very close to 0, so the function is neither increasing nor decreasing at that point\n",
    "\n",
    "A more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eea035",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "f = a*b+c\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e2f75",
   "metadata": {},
   "source": [
    "Let see how the function changes with respect to the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f10622",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "df_da = (((a+h)*b + c) - (a*b + c)) / h\n",
    "df_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a261",
   "metadata": {},
   "source": [
    "This -3 means that the function value decreases proportional to 3 times the increment in 'a'. \n",
    "\n",
    "Lets see some other increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db = ((a*(b+h) + c) - (a*b + c)) / h\n",
    "df_dc = ((a*b+c+h) - (a*b+c)) / h\n",
    "\n",
    "df_da, df_db, df_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1861539",
   "metadata": {},
   "source": [
    "You can check the values are very close to the analytical \"partial derivatives\"\n",
    "How can I modify the parameters a, b, and c if I want to increase the value of f?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c14a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "a*b+c, (a-h)*(b+h)+(c+h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = -0.01\n",
    "a*b+c, (a-h)*(b+h)+(c+h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926fff1",
   "metadata": {},
   "source": [
    "# Gradient descend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5e86e",
   "metadata": {},
   "source": [
    "Lets create a class for storing values, adding operations and parameters of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "a = Value(2)\n",
    "b = Value(-3.0)\n",
    "d = (a*b)\n",
    "d, d._prev, d._op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf74a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let create a way to visualize the expression\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{data: %s}\" % (n.data), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a699b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = Value(2)\n",
    "b = Value(-3.0)\n",
    "c = Value(10)\n",
    "d = a*b + c\n",
    "\n",
    "draw_dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852e0e0",
   "metadata": {},
   "source": [
    "Lets add some labels to the nodes, in order to identify them with ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718249a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%s|data: %s}\" % (n.label, n.data), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label='d'\n",
    "f = Value(-2, label='f')\n",
    "L = d*f; L.label='L'\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1190b2",
   "metadata": {},
   "source": [
    "You can see that we have a matematical expression linking L with four free parameters: a, b, c, and f. We are now to run backpropagation, trying to increase the value of L by changing the values of the free parameters.\n",
    "- For every single value we are going to calculate the derivative, using the chain rule. Using this, we will know how to change the values for increasing L \n",
    "\n",
    "In order to do so, we will add a property in Value to hold the derivative of L with respect to that value. We will name this property 'grad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca487d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%s|data: %.4f|grad:%.4f}\" % (n.label, n.data, n.grad), \n",
    "             shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "a = Value(2, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label='d'\n",
    "f = Value(-2, label='f')\n",
    "L = d*f; L.label='L'\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d50ba",
   "metadata": {},
   "source": [
    "We start back to front, manually. We started by L\n",
    "\n",
    "dL/dL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7abed6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L.grad = 1\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28c6d4",
   "metadata": {},
   "source": [
    " Now, lets calculate the derivatives with respect to f and d. Since L = f*d:\n",
    " \n",
    " dL/df = d\n",
    " \n",
    " dL/dd = f\n",
    " \n",
    " Lets check by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2+h, label='f')  # HERE    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d2961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    d.data += h                 # HERE\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since they are correct, we will set the values\n",
    "d.grad = f.data\n",
    "f.grad = d.data\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a97dd",
   "metadata": {},
   "source": [
    "Now, lets move to the previous values, e and c. We now need to calculate dL/de and dL/dc. From the chain rule we know that:\n",
    "\n",
    "dL/de = dL/dd*dd/de\n",
    "- We already calculated dL/dd, which is -2\n",
    "- Since d = e+c, dd/de=1\n",
    "- As a result **dL/de = -2 * 1 = -2**\n",
    "\n",
    "In a similar waw, **dL/dc = -2**\n",
    "\n",
    "**Note**: The + node only passes the gradient without modification\n",
    "\n",
    "Lets check numerically if it is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436451f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    e.data += h              # HERE\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792df90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10+h, label='c') # HERE\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c2953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After the check we know it is correct, so we update the .grad property\n",
    "e.grad = -2\n",
    "c.grad = -2\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e7ad2",
   "metadata": {},
   "source": [
    "Now, lets calculate the final one, the gradient for *a* and *b*, also using the chain rule.\n",
    "\n",
    "- dL/da = dL/de * de/da\n",
    "- dL/de is already known, -2\n",
    "- Since e = a*b, then de/da = b\n",
    "\n",
    "Finally, dL/da = -2 * -3 = 6\n",
    "\n",
    "Similarly, dL/db = dL/de*de/db = -2 * a = -2 * 2 = -4\n",
    "\n",
    "Lets check it numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdf942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2+h, label='a') # HERE\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14893d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0+h, label='b') # HERE\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df8508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since they are correct, we updated the .grad properties.\n",
    "a.grad = 6\n",
    "b.grad = -4\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557fba6",
   "metadata": {},
   "source": [
    "**Note**: There are some parameters that we can change, like a, b, c, and f, while the others are calculated, so cannot be changed.\n",
    "\n",
    "Now, let use the gradient in order to increase the value of L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, bg, cg, fg = a.grad, b.grad, c.grad, f.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_by_hand(h = 0.01):\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2 + ag * h, label='a')\n",
    "    b = Value(-3.0 + bg * h, label='b')\n",
    "    c = Value(10 + cg * h, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2 + fg * h, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L2 = L.data\n",
    "\n",
    "    print(L1, L2, L2-L1)\n",
    "    \n",
    "eval_by_hand(0.01)\n",
    "eval_by_hand(-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af34270",
   "metadata": {},
   "source": [
    "Note, it is increased. This is the backpropagation algorithm in action! \n",
    "\n",
    "Lets move to Neural Networks.\n",
    "\n",
    "A neural network has:\n",
    "- Neurons\n",
    "    - Weights\n",
    "    - Bias (neuron default activation in absence of inputs)\n",
    "    - Activation function: \n",
    "        - Introduce nonlinearities, frequently squashing the neuron output\n",
    "        \n",
    "There are some common activation function, like the tanh and ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2437561",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.1)\n",
    "plt.plot(x, np.tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec11d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.1)\n",
    "\n",
    "plt.plot(x, np.where(x > 0, x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c99b45",
   "metadata": {},
   "source": [
    "Lets create a sistem with two neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2969fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.7, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "\n",
    "draw_dot(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a30b9",
   "metadata": {},
   "source": [
    "Lets add the code for the activation function. \n",
    "- Since in our Value we only sums and products, we need a new node for the tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d5af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ed602",
   "metadata": {},
   "source": [
    "Note than here the efect of tanh is minimal, but lets change the bias to 10 we would see the squashing efect of tanh. Now, lets run backpropagation on the neuron.\n",
    "\n",
    "**Note**. While training the neuron, the only parameters that we can change are the weights and biases, because the training examples are fixed.\n",
    "\n",
    "First, since do/do=1, lets set that in the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ff24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d428f",
   "metadata": {},
   "source": [
    "dtanh(x)/dx = 1 - tanh(x)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225551f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.grad = 1 - (o.data)**2\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify():\n",
    "    h = 0.001\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    n.data += h\n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "verify()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd9c76",
   "metadata": {},
   "source": [
    "do/db = do/dn*dn/db = 0.5 * 1\n",
    "\n",
    "do/d(x1w1+x2w2) = do/dn * dn/d(x1w1+x2w2) = 0.5 * 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f902d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.grad = 0.5\n",
    "x1w1x2w2.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce22b8",
   "metadata": {},
   "source": [
    "do/dx2w2 = do/d(x1w1+x2w2) * d(x1w1+x2w2)/dx2w2\n",
    "    = 0.5 * 1 = 0.5\n",
    "    \n",
    "do/dx1w1 = do/d(x1w1+x2w2) * d(x1w1+x2w2)/dx1w1\n",
    "    = 0.5 * 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb7d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x2w2.grad = 0.5\n",
    "x1w1.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9efa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify():\n",
    "    h = 0.001\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x1w1.data += h\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b;\n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "verify()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57ead2",
   "metadata": {},
   "source": [
    "do/dw1 = do/dx1w1 * dx1w1/dw1 = 0.5*x1 = 0.5*2 = 1\n",
    "\n",
    "do/dx1 = do/dx1w1 * dx1w1/dx1 = 0.5*w1 = 0.5*-3 = -1.5\n",
    "\n",
    "do/dw2 = do/dx2w2 * dx2w2/dw2 = 0.5*x2 = 0.5*0 = 0\n",
    "\n",
    "d0/dx2 = do/dx2w2 * dx2w2/dx2 = 0.5*w2 = 0.5*1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.grad = 1\n",
    "x1.grad = -1.5\n",
    "w2.grad = 0\n",
    "x2.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789dc939",
   "metadata": {},
   "source": [
    "Now, the parameters we can modify in order to increase the value of the function are the weights and biases.\n",
    "\n",
    "Lets modify them to icrease the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed59bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w1 = 1\n",
    "grad_w2 = 0\n",
    "grad_b = 0.5\n",
    "\n",
    "def modify(h):\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0 + h*grad_w1), Value(1.0+h*grad_w2)\n",
    "    b = Value(6.8812735870195432 + h*grad_b)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    return L1, L2, L2-L1\n",
    "\n",
    "print(modify(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da0c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(modify(-0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec758662",
   "metadata": {},
   "source": [
    "You can see that applying backpropagation is very simple, but tedious. Lets move to automatically calculate the gradient (autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = 1.0 * out.grad\n",
    "            other.grad = 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * other.data\n",
    "            other.grad = out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6737994",
   "metadata": {},
   "source": [
    "Lets put it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773341f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.grad = 1\n",
    "o._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09144c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4dbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1w1x2w2._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36e5f0",
   "metadata": {},
   "source": [
    "So, in order to perform everything automatically, we need to get the list of all nodes, moving from 'o' to the first nodes. \n",
    "\n",
    "We perform this using a topological sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c045be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo = []\n",
    "visited = set()\n",
    "def build_topo(v):\n",
    "    if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "            build_topo(child)\n",
    "        topo.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_topo(o)\n",
    "topo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c115ce6",
   "metadata": {},
   "source": [
    "As you can see, in this order, all the nodes refered by a given node are always traveled **before** the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1\n",
    "build_topo(o)\n",
    "for t in topo[::-1]:\n",
    "    t._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fa16d",
   "metadata": {},
   "source": [
    "Lets put this inside a method in the Value class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34496bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = 1.0 * out.grad\n",
    "            other.grad = 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * other.data\n",
    "            other.grad = out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8b98c",
   "metadata": {},
   "source": [
    "There is a subtle problem in the code. Lets see in an example. \n",
    "\n",
    "Which is the expected value of the grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a; b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4a907",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "db/da = d(a+a)/da = da/da + da/da = 1 + 1 = 2\n",
    "\n",
    "The problem with our code is that grad values are set, instead of updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610298e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a; b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabb81e",
   "metadata": {},
   "source": [
    "Lets see it on a more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e5be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b; d.label='d'\n",
    "e = a + b; e.label='e'\n",
    "f = d * e; f.label='f'\n",
    "\n",
    "f.backward()\n",
    "draw_dot(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be192117",
   "metadata": {},
   "source": [
    "You can check by hand that everything is working perfectly. Lets add a final manual check!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    h = 0.001\n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L1 = f.data\n",
    "    \n",
    "    a = Value(-2.0+h, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L2 = f.data \n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    h = 0.001\n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L1 = f.data\n",
    "    \n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0+h, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L2 = f.data \n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e31d24",
   "metadata": {},
   "source": [
    "## Decomposing tanh by its components\n",
    "\n",
    "tanh(x) = (e** (2*x)-1) / (e** (2*x)+1)\n",
    "\n",
    "Lets modify Value in order to use use the tanh definition. We introduce some modifications in order to deal with adding a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        print(self.data, other.data)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d2760",
   "metadata": {},
   "source": [
    "Lets add now the exponentiation method, with the proper derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        print(self.data, other.data)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b323bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "a.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488d087",
   "metadata": {},
   "source": [
    "For the division, we will introduce something more general.\n",
    "\n",
    "a / b = a * (1/b) = a * b**-1\n",
    "\n",
    "So, we need the power operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        print(self.data, other.data)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        assert isinstance(exponent, (int, float)), \"Only support int and float for now\"\n",
    "        out = Value(self.data ** exponent, (self, ), f'**{exponent}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * exponent * self.data ** (exponent -1)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(4.0)\n",
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7127924",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(8.0)\n",
    "b = Value(2.0)\n",
    "a/b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c5edf",
   "metadata": {},
   "source": [
    "For completness, lets add the substraction and negation operands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        assert isinstance(exponent, (int, float)), \"Only support int and float for now\"\n",
    "        out = Value(self.data ** exponent, (self, ), f'**{exponent}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * exponent * self.data ** (exponent -1)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb27988",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(8.0)\n",
    "b = Value(5.0)\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ed599",
   "metadata": {},
   "outputs": [],
   "source": [
    "-a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e39fa",
   "metadata": {},
   "source": [
    "Lets go back to our example, in order to modify the tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b63da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c229c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh(x) = (e** (2x)-1) / (e**(2*x)+1)\n",
    "\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "\n",
    "# o = n.tanh(); o.label='o'\n",
    "ex = (n*2).exp(); ex.label='exp(2**n)'\n",
    "o = (ex -1) / (ex + 1); o.label=o\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3a7ad",
   "metadata": {},
   "source": [
    "This example shows a very important point: **The level of details in the components is up to your needs. You can assemble complex behaviours as units, and you only need to provide the gradient of the complex unit**.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c27e16",
   "metadata": {},
   "source": [
    "# Neural Networs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # return tanh(w * x + b)\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Neuron(3)\n",
    "n.w, n.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "n([1, -1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0354aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e47abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Layer(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795da4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l([1, -1, 2, 0, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40138a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs if len(outs) > 1 else outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856c99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = Layer(5, 1)\n",
    "l([1, -1, 2, 0, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e90efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, nin, nout):\n",
    "        sz = [nin] + nout\n",
    "        self.layers = [Layer(zi, zo) for zi, zo in zip(sz, sz[1:])]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP(3, [4, 4, 1])\n",
    "inp = [-1, 0.5, 3]\n",
    "m(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365790e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_dot(m(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a7b8c",
   "metadata": {},
   "source": [
    "With all the available tools, lets train the network with a simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, -1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc868e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypred = [m(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e216c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(yout-ygt)**2 for ygt, yout in zip(ys, ypred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd910ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sum([(yout-ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4bd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.layers[0].neurons[1].w[1].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a4421",
   "metadata": {},
   "source": [
    "Remember that we want to **minimize** the loss, so we need to **substract** the gradient.\n",
    "\n",
    "Now, we need to add a mechanisms to track which nodes contains values that can be updated, in contrast with input nodes and temporary nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:    \n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # return tanh(w * x + b)\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "\n",
    "class Layer:    \n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs if len(outs) > 1 else outs[0]\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "    \n",
    "\n",
    "class MLP:    \n",
    "    def __init__(self, nin, nout):\n",
    "        sz = [nin] + nout\n",
    "        self.layers = [Layer(zi, zo) for zi, zo in zip(sz, sz[1:])]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP(3, [4, 4, 1])\n",
    "m.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in m.layers:\n",
    "    print(len(l.parameters()))\n",
    "print(\"-\"*10)\n",
    "print(len(m.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee6264",
   "metadata": {},
   "source": [
    "layer1: 3 * 4 + 4\n",
    "\n",
    "layer2: 4 * 4 + 4\n",
    "\n",
    "layer3: 4 * 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954da4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [m(x) for x in xs]\n",
    "loss = sum([(yout-ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "print(loss)\n",
    "\n",
    "for p in m.parameters():\n",
    "    p.grad = 0\n",
    "loss.backward()\n",
    "\n",
    "learning_rate = 0.01\n",
    "for p in m.parameters():\n",
    "    p.data -= learning_rate * p.grad\n",
    "\n",
    "    \n",
    "ypred = [m(x) for x in xs]\n",
    "loss = sum([(yout-ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a7546",
   "metadata": {},
   "source": [
    "# Example 1. Simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        \n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value({self.label}, data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            # print(f'    Calling _backward() (+) on {self}')\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # print(f'    Calling _backward() (*) on {self}')\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        \n",
    "        assert isinstance(other, (int, float)), \"Called __pow__ with non-int/float argument!\"\n",
    "        \n",
    "        out = Value(self.data**other, (self, ), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            # power rule combined with chain rule\n",
    "            self.grad += other * self.data**(other - 1) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "     \n",
    "    def tanh(self):\n",
    "\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            # print(f'    Calling _backward() on {self}')\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # d/dx(e^x) = e^x\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "    \n",
    "        def topological_sort(v):\n",
    "\n",
    "            ordering = []\n",
    "            visited = set()\n",
    "\n",
    "            def grow_topological(v):\n",
    "                if v not in visited:\n",
    "                    visited.add(v)\n",
    "                    for child in v._prev:\n",
    "                        grow_topological(child)\n",
    "                    ordering.append(v)    \n",
    "\n",
    "            grow_topological(v)\n",
    "\n",
    "            return ordering\n",
    "    \n",
    "        self.grad = 1.0\n",
    "\n",
    "        topo = topological_sort(self)\n",
    "        \n",
    "        for v in reversed(topo):\n",
    "            # print(f'Calling _backward() on {v}')\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99761cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        \n",
    "        # Note that our __init__ doesn't offer args for\n",
    "        # weight or bias\n",
    "        \n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(num_inputs)]\n",
    "\n",
    "        self.b = Value(0)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "\n",
    "        # self.w * x + self.b\n",
    "        \n",
    "        # Supports an arbitrary-length vector as the neuron's input\n",
    "        \n",
    "        # Note that we can pass our neuron's bias as the `start` arg for sum()\n",
    "        \n",
    "        # activation = sum(w_i * x_i for w_i, x_i in zip(self.w, x)) + self.b\n",
    "        activation = sum((w_i * x_i for w_i, x_i in zip(self.w, x)), self.b)\n",
    "        \n",
    "        # Using tanh() as our simple activation function for the neuron\n",
    "        \n",
    "        output = activation.tanh()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "    \n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        \n",
    "        # Each output will exist as a single neuron, which takes\n",
    "        # `num_inputs` inputs;\n",
    "        \n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_outputs)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Calling our neurons with a vector input yields a vector output\n",
    "        # of the same dimension; we'll convert unary vectors to scalars\n",
    "        \n",
    "        outputs = [n(x) for n in self.neurons]\n",
    "        \n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        # params = []\n",
    "        # for neuron in self.neurons:\n",
    "        #     params.extend(neuron.parameters())\n",
    "        # return params\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    \n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        \n",
    "        # Layers appear in sequence, meaning that one layer's output count\n",
    "        # is equivalent to the next layer's input count; any two neighboring\n",
    "        # values in `num_outputs` tells us the shape of a corresponding layer\n",
    "        #\n",
    "        # Based on this, we can calculate the total number of neurons in\n",
    "        # our perceptron:\n",
    "        # \n",
    "        #   The first layer (i=0) receives `num_inputs` inputs and\n",
    "        #   yields `num_outputs[0]` outputs\n",
    "        # \n",
    "        #   The i-th layer (i>0) receives `num_outputs[i-1]` inputs and\n",
    "        #   yields `num_outputs[i]` outputs\n",
    "        #\n",
    "        # We can generalize this by combining `num_inputs` and `num_outputs`\n",
    "        # into a single addressable lookup list:\n",
    "        \n",
    "        sizes = [num_inputs] + num_outputs\n",
    "        \n",
    "        lookup = range(len(num_outputs))\n",
    "        \n",
    "        self.layers = [Layer(sizes[i], sizes[i+1]) for i in lookup]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # `x` contains a `num_inputs`-size input vector; we will pass `x`\n",
    "        # through each of our percetron's layers, mutating it with each pass\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Return the final result (output)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec023c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "# Each sample input has one desired output that we want our MLP to predict\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9abde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afa2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing out the training loop\n",
    "\n",
    "training_cycles = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(training_cycles):\n",
    "\n",
    "    # forward pass\n",
    "    y_predicted = [P(x) for x in xs]\n",
    "\n",
    "    # recalculate loss\n",
    "    loss = sum([(y_output - y_ground_truth)**2 for y_ground_truth, y_output in zip(ys, y_predicted)])\n",
    "\n",
    "    # backward pass\n",
    "    \n",
    "    # COMMON BUG: Forgetting to zero-out your gradients before\n",
    "    # running the next backward pass\n",
    "    \n",
    "    for p in P.parameters():\n",
    "        p.grad = 0\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # gradient updates\n",
    "    for p in P.parameters():\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "    # print current loss\n",
    "    print(f'i={i}: loss={loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c32987",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809efac",
   "metadata": {},
   "source": [
    "# Solving moon problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51055edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model \n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization\n",
    "n_epoch = 70\n",
    "\n",
    "for k in range(n_epoch):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss()\n",
    "    \n",
    "    # backward\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/n_epoch\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_lectures]",
   "language": "python",
   "name": "conda-env-python_lectures-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
