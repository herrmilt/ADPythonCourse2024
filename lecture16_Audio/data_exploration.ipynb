{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eee1385",
   "metadata": {},
   "source": [
    "ideas taken from: https://www.kaggle.com/code/salimhammadi07/esc-50-environmental-sound-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed8679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls data/background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1243b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio('data/background/background_00.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7dcf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio('data/chainsaw/chainsaw_00.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8773e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('data/chainsaw/chainsaw_00.wav')\n",
    "print('y:', y, '\\n')\n",
    "print('y shape:', np.shape(y), '\\n')\n",
    "print('Sample Rate (KHz):', sr, '\\n')\n",
    "\n",
    "# The duration is equal to the number of frames divided by the framerate\n",
    "print('Duration of the audio file:', np.shape(y)[0]/sr, 'second')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845ff2d",
   "metadata": {},
   "source": [
    "Load all sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19237e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "all_waves = {}\n",
    "base_path = \"data\"\n",
    "for category in ['background', 'chainsaw', 'engine', 'storm']:\n",
    "    all_waves[category] = []\n",
    "    for audio_file in os.listdir(os.path.join(base_path, category)):\n",
    "        file_name = os.path.join(base_path, category, audio_file)\n",
    "        y, sr = librosa.load(file_name)\n",
    "        all_waves[category].append((y, sr, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde41013",
   "metadata": {},
   "source": [
    "# Sound Waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57651a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr, file_name = all_waves['background'][0]\n",
    "Audio(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1ada9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "librosa.display.waveshow(y[10000:11000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a85f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_in_plots(fn, cant_per_row=3, xlabel=None, ylabel=None):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    idx = 1\n",
    "    for cat_name, items in all_waves.items():\n",
    "        for y, sr, _ in items[:cant_per_row]:\n",
    "            plt.subplot(4,cant_per_row,idx)\n",
    "            idx += 1\n",
    "            fn(y, sr)\n",
    "            if xlabel:\n",
    "                plt.xlabel(xlabel)\n",
    "            if ylabel:\n",
    "                plt.ylabel(ylabel)\n",
    "            plt.title(cat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _waveshow(y, sr):\n",
    "    librosa.display.waveshow(y)\n",
    "    \n",
    "show_in_plots(_waveshow, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f3d31",
   "metadata": {},
   "source": [
    "# Visualize Audio : Fourier Transform\n",
    "\n",
    "The Fourier transform is a mathematical technique used to decompose a signal into its constituent frequency components. It is widely used in audio signal processing to analyze, filter and manipulate sound signals.\n",
    "\n",
    "The Fourier transform of a time-domain signal, such as an audio signal, produces a frequency-domain representation of the signal. This representation shows the relative amplitudes of the different frequency components that make up the signal. This information is useful for understanding the characteristics of the sound, such as its pitch and timbre, and for filtering or modifying specific frequency ranges.\n",
    "\n",
    "There are different types of Fourier transforms, the most common is the discrete Fourier transform (DFT), which is used to convert a discrete-time signal into a discrete-frequency representation. The DFT requires a large amount of computation, so in practice, the fast Fourier transform (FFT) algorithm is often used to efficiently calculate the DFT.\n",
    "\n",
    "The short-time Fourier transform (STFT) is a variation of the DFT that is used to analyze audio signals. It breaks the audio signal into short segments and applies the DFT to each segment, providing a time-frequency representation of the signal. This is useful for analyzing the frequency content of a sound over time, and for tasks such as pitch detection and audio compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6632a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr, file_name = all_waves['chainsaw'][0]\n",
    "Audio(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default FFT window size\n",
    "n_fft = 2048 # FFT window size\n",
    "hop_length = 512 # number audio of frames between STFT columns \n",
    "\n",
    "X = np.abs(librosa.stft(y, n_fft = n_fft, hop_length = hop_length))\n",
    "plt.plot(X)\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df243b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fftshow(y, sr):\n",
    "    X = np.abs(librosa.stft(y, n_fft = n_fft, hop_length = hop_length))\n",
    "    plt.plot(X)\n",
    "    \n",
    "show_in_plots(_fftshow, xlabel=\"Frequency\", ylabel=\"Amplitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8949ce",
   "metadata": {},
   "source": [
    "# Spectrogram\n",
    "\n",
    "A spectrogram is a time-frequency representation of a signal, such as an audio signal. It is a graphical representation of the frequency content of a signal over time, and is often used to visualize and analyze audio signals.\n",
    "\n",
    "A spectrogram is typically represented as a 2D image, with the x-axis representing time, the y-axis representing frequency, and the intensity of the color or grayscale representing the amplitude of the frequency component at that point in time.\n",
    "\n",
    "The spectrogram is calculated by applying the Short-Time Fourier Transform (STFT) to the audio signal, which breaks the audio into short segments and applies the Fourier transform to each segment. This produces a set of complex numbers representing the frequency content of the audio for each segment, which are then plotted in the spectrogram.\n",
    "\n",
    "A spectrogram can be useful for visualizing the frequency content of a sound over time, and for identifying patterns in the audio signal, such as pitch, timbre, and transient events. It can also be used to analyze the characteristics of different sounds, such as the spectral envelope or the harmonic structure, and to segment an audio file into different sound events.\n",
    "\n",
    "A spectrogram can be used in many audio-related tasks, such as speech recognition, audio source separation, and audio event detection, and it is an essential tool in the field of audio signal processing.\n",
    "\n",
    "We can display a spectrogram using. librosa.display.specshow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c31991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expectrogramshow(y, sr):\n",
    "    X = librosa.stft(y)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar()\n",
    "    \n",
    "show_in_plots(_expectrogramshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ae0e0",
   "metadata": {},
   "source": [
    "# Mel Spectrogram\n",
    "## The Mel Scale\n",
    "\n",
    "Studies have shown that humans do not perceive frequencies on a linear scale. We are better at detecting differences in lower frequencies than higher frequencies. For example, we can easily tell the difference between 500 and 1000 Hz, but we will hardly be able to tell a difference between 10,000 and 10,500 Hz, even though the distance between the two pairs are the same.\n",
    "\n",
    "In 1937, Stevens, Volkmann, and Newmann proposed a unit of pitch such that equal distances in pitch sounded equally distant to the listener. This is called the mel scale. We perform a mathematical operation on frequencies to convert them to the mel scale.\n",
    "\n",
    "## The Mel Spectrogram\n",
    "\n",
    "- A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale.\n",
    "\n",
    "- A mel spectrogram logarithmically renders frequencies above a certain threshold (the corner frequency). For example, in the linearly scaled spectrogram, the vertical space between 1,000 and 2,000Hz is half of the vertical space between 2,000Hz and 4,000Hz. In the mel spectrogram, the space between those ranges is approximately the same. This scaling is analogous to human hearing, where we find it easier to distinguish between similar low frequency sounds than similar high frequency sounds.\n",
    "\n",
    "- A mel spectrogram computes its output by multiplying frequency-domain values by a filter bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr, file_name = all_waves['background'][1]\n",
    "Audio(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbe130",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = librosa.effects.trim(y)\n",
    "XS = librosa.feature.melspectrogram(y=X, sr=sr)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.colorbar() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr, file_name = all_waves['storm'][2]\n",
    "Audio(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda27488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = librosa.effects.trim(y)\n",
    "XS = librosa.feature.melspectrogram(y=X, sr=sr)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.colorbar() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _melexpectrogramshow(y, sr):\n",
    "    X, _ = librosa.effects.trim(y)\n",
    "    XS = librosa.feature.melspectrogram(y=X, sr=sr)\n",
    "    Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar()   \n",
    "    \n",
    "show_in_plots(_melexpectrogramshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a2d83",
   "metadata": {},
   "source": [
    "## Filtering Mel Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f14308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = -60\n",
    "\n",
    "def _filtmelexpectrogramshow(y, sr):\n",
    "    X, _ = librosa.effects.trim(y)\n",
    "    XS = librosa.feature.melspectrogram(y=X, sr=sr)\n",
    "    Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "    Xdb[Xdb < threshold] = threshold\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar()   \n",
    "    \n",
    "show_in_plots(_filtmelexpectrogramshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf0453",
   "metadata": {},
   "source": [
    "## Mel spectrograms parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49471d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr, _ = all_waves['storm'][2]\n",
    "print(y.shape, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ba6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "XS = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128, fmin=0.0, fmax=None)\n",
    "print(XS.shape)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45bd0b",
   "metadata": {},
   "source": [
    "The resultant array shape is (128, 431).\n",
    "- 128 is the number of mel bands\n",
    "- 431 is the number of time slots calculated. 430 aprox 220500/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e49304",
   "metadata": {},
   "outputs": [],
   "source": [
    "220500/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of mel bands. The more bands, the more filter details\n",
    "XS = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=40)\n",
    "print(XS.shape)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase hop_lengths\n",
    "XS = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=4096, n_mels=128, fmin=0.0, fmax=None)\n",
    "print(XS.shape)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease hop_lengths\n",
    "XS = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=128, n_mels=128, fmin=0.0, fmax=None)\n",
    "print(XS.shape)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353154ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing frequencies\n",
    "fmin = 2000\n",
    "fmax = 4000\n",
    "XS = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128, fmin=fmin, fmax=fmax)\n",
    "print(XS.shape)\n",
    "Xdb = librosa.amplitude_to_db(XS, ref=np.max)\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', hop_length=512, y_axis='mel', fmin=fmin, fmax=fmax)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181f746",
   "metadata": {},
   "source": [
    "# Other features\n",
    "## Zero Crossing Rate\n",
    "\n",
    "Zero-Crossing Rate: The zero-crossing rate (ZCR) is the rate at which a signal transitions from positive to zero to negative or negative to zero to positive. Its value has been extensively used in both speech recognition and music information retrieval for classifying percussive sounds.\n",
    "\n",
    "The zero-crossing rate can be utilized as a basic pitch detection algorithm for monophonic tonal signals. Voice activity detection (VAD), which determines whether or not human speech is present in an audio segment, also makes use of zero-crossing rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr, file_name = all_waves['engine'][1]\n",
    "Audio(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93618f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom=50\n",
    "y = y[0:zoom]\n",
    "librosa.display.waveshow(y)\n",
    "crossings = librosa.zero_crossings(y, pad=False)\n",
    "t = np.linspace(0, 0.0022, num=zoom)\n",
    "plt.scatter(t[crossings],y[crossings]*0, color='r',linewidth=7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zerocross(y, sr):\n",
    "    y = y[0:zoom]\n",
    "    librosa.display.waveshow(y)\n",
    "    crossings = librosa.zero_crossings(y, pad=False)\n",
    "    t = np.linspace(0, 0.0022, num=zoom)\n",
    "    plt.scatter(t[crossings],y[crossings]*0, color='r',linewidth=7.0)\n",
    "\n",
    "show_in_plots(_zerocross, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09c71d",
   "metadata": {},
   "source": [
    "## Harmonics and Percussive\n",
    "\n",
    "Decompose an audio time series into harmonic and percussive components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d0e77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _harmperc(y, sr):\n",
    "    y_harm, y_perc = librosa.effects.hpss(y)\n",
    "    plt.plot(y_harm, alpha=0.4);\n",
    "    plt.plot(y_perc, color = 'purple', alpha=0.8);\n",
    "\n",
    "show_in_plots(_harmperc, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ff3a5",
   "metadata": {},
   "source": [
    "## Tempo BMP (Beats Per Minute)\n",
    "\n",
    "What's a \"beat?\" People commonly use the word \"beat\" to mean \"a pattern (or rhythm) played by drums.\" The thing you're making when you create and play patterns in these lessons is \"a beat.\"\n",
    "\n",
    "But, confusingly, there's another use of the word \"beat,\" which means \"a regular, repeating pulse that underlies a musical pattern.\" People tap their foot along with \"the beat\" in this context.\n",
    "\n",
    "Tempo The speed at which your patterns play back is called the tempo. Tempo is measured in beats per minute or BPM. So if we talk about a piece of music being \"at 120 BPM,\" we mean that there are 120 beats (pulses) every minute.\n",
    "\n",
    "Some types of musical patterns have a very clear underlying beat, while others have a more subtle or implied one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482404c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_name, items in all_waves.items():\n",
    "    print(cat_name)\n",
    "    for y, sr, file_name in items[:4]:\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        print('-', tempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cdfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempos = {}\n",
    "for cat_name, items in all_waves.items():\n",
    "    values = [librosa.beat.beat_track(y=y, sr=sr)[0] for y, sr, _ in items]\n",
    "    print(f\"{cat_name}, mean: {np.mean(values):.2f}, stdev: {np.std(values):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa112e60",
   "metadata": {},
   "source": [
    "## Spectral Centroid\n",
    "\n",
    "The spectral centroid indicates at which frequency the energy of a spectrum is centered upon or in other words It indicates where the ” center of mass” for a sound is located. This is like a weighted mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75006849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn \n",
    "\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "\n",
    "def _spectcentro(y, sr):\n",
    "    librosa.display.waveshow(y, alpha=0.4)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "    frames = range(len(spectral_centroids))\n",
    "    t = librosa.frames_to_time(frames)\n",
    "    plt.plot(t, normalize(spectral_centroids), color='r')\n",
    "\n",
    "show_in_plots(_spectcentro, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6751a",
   "metadata": {},
   "source": [
    "## Spectral Rolloff\n",
    "\n",
    "The spectral rolloff is a measure of the shape of the spectrum of an audio signal. It is defined as the frequency below which a certain percentage of the total energy of the signal lies. The roll-off point is often expressed as a percentage of the total energy, such as 85% or 95%.\n",
    "\n",
    "The spectral rolloff can provide information about the tonality of a sound, as sounds that are more tonal will have a lower rolloff point than sounds that are more noise-like. For example, a piano playing a sustained note will have a lower rolloff point than a snare drum hit.\n",
    "\n",
    "The spectral rolloff can be calculated by first computing the power spectrum of the audio signal, and then finding the frequency below which a certain percentage of the total energy of the signal lies. The spectral rolloff can be computed for different percentage values, and the result can be a single value, or a set of values for different percentages.\n",
    "\n",
    "The spectral rolloff can be used in various audio-related tasks, such as music genre classification, sound event detection, and speech analysis, as it can be a useful feature to distinguish different audio classes. It can also be used in combination with other features, such as Mel-Frequency Cepstral Coefficients (MFCCs) or Chroma feature, to improve the performance of audio classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f395804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _spectrolloff(y, sr):\n",
    "    librosa.display.waveshow(y,alpha=0.4)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "    frames = range(len(spectral_rolloff))\n",
    "    t = librosa.frames_to_time(frames)\n",
    "    plt.plot(t, normalize(spectral_rolloff), color='r')\n",
    "\n",
    "show_in_plots(_spectrolloff, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fee030",
   "metadata": {},
   "source": [
    "## Spectral Bandwidth\n",
    "\n",
    "The spectral bandwidth is defined as the width of the band of light at one-half the peak maximum (or full width at half maximum [FWHM]) and is represented by the two vertical red lines and λSB on the wavelength axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a347385",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _spectbandwidth(y, sr):\n",
    "    librosa.display.waveshow(y, alpha=0.4)\n",
    "    spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(y=y+0.01, sr=sr)[0]\n",
    "    spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(y=y+0.01, sr=sr, p=3)[0]\n",
    "    spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(y=y+0.01, sr=sr, p=4)[0]\n",
    "    frames = range(len(spectral_bandwidth_2))\n",
    "    t = librosa.frames_to_time(frames)\n",
    "    plt.plot(t, normalize(spectral_bandwidth_2), color='r')\n",
    "    plt.plot(t, normalize(spectral_bandwidth_3), color='g')\n",
    "    plt.plot(t, normalize(spectral_bandwidth_4), color='y')\n",
    "    plt.legend(('p = 2', 'p = 3', 'p = 4'))\n",
    "\n",
    "show_in_plots(_spectbandwidth, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1ea83",
   "metadata": {},
   "source": [
    "## Chroma feature\n",
    "\n",
    "Chroma feature is a representation of the harmonic structure of an audio signal. It is a powerful feature for music analysis and can be used for tasks such as music genre classification, chord recognition, and tonality analysis.\n",
    "\n",
    "A Chroma feature is calculated by first transforming the audio signal into the frequency domain using a Fourier transform. The signal is then mapped into a new feature space called the Chroma space, which consists of 12 bins corresponding to the 12 distinct semitones of Western music.\n",
    "\n",
    "Each bin represents the energy of the audio signal at a specific pitch class (C, C#, D, D#, etc.) and is calculated by summing the energy of all the notes in the signal that belong to that pitch class. The resulting Chroma feature is a 12-dimensional vector, where each dimension represents the energy of the audio signal at a specific pitch class.\n",
    "\n",
    "The Chroma feature is robust to changes in tempo and instrumentation, which makes it a useful feature for tasks such as music genre classification and chord recognition. It can also be used in conjunction with other features, such as Mel-Frequency Cepstral Coefficients (MFCCs) or spectral rolloff, to improve the performance of audio classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chromafeat(y, sr):\n",
    "    chromagram = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "    librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\n",
    "\n",
    "show_in_plots(_chromafeat, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74b8f7",
   "metadata": {},
   "source": [
    "## Mel-Frequency Cepstral Coefficients(MFCCs)\n",
    "\n",
    "Mel-Frequency Cepstral Coefficients (MFCCs) are a set of features commonly used in speech and music processing applications to represent the spectral characteristics of an audio signal. They are based on the human perception of sound and are designed to capture the spectral envelope of the audio signal, which is the shape of the signal's power spectrum over time.\n",
    "\n",
    "The MFCCs are calculated in several steps:\n",
    "\n",
    "- The audio signal is transformed into the frequency domain using a Fourier transform.\n",
    "- The Mel scale is applied to the frequency axis of the signal, which approximates the non-linear frequency response of the human ear.\n",
    "- The logarithm of the energy in each Mel-frequency bin is taken to obtain the Mel-frequency spectrogram.\n",
    "- A Discrete Cosine Transform (DCT) is applied to the Mel-frequency spectrogram to obtain the MFCCs.\n",
    "\n",
    "The result of these steps is a set of coefficients that represent the spectral envelope of the audio signal. The number of MFCCs used in a given application can vary, but typically, between 12 and 40 coefficients are used.\n",
    "\n",
    "MFCCs are robust to variations in the audio signal, such as changes in pitch, speed, and noise. They are widely used in speech and music processing tasks such as speech recognition, music genre classification, and speaker identification. They can also be used in conjunction with other features, such as Chroma feature or Spectral rolloff, to improve the performance of audio classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec7ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _mfccfeat(y, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    \n",
    "show_in_plots(_mfccfeat, xlabel=\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff91a2",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data. This can be useful in situations where the amount of available data is limited, such as with the ESC-50 dataset.\n",
    "\n",
    "Data augmentation can be used to improve the robustness and generalization of machine learning models by introducing variation into the training data. This can help the model to learn more robust features that are less dependent on specific variations in the data.\n",
    "\n",
    "There are several types of data augmentation that can be applied to the ESC-50 dataset:\n",
    "\n",
    "- Time stretching : This technique changes the duration of the audio signal by speeding it up or slowing it down. This can be useful for simulating variations in the tempo of the audio.\n",
    "\n",
    "- Pitch shifting: This technique changes the pitch of the audio signal by shifting it up or down. This can be useful for simulating variations in the pitch of the audio.\n",
    "\n",
    "- Volume scaling: This technique changes the volume of the audio signal by scaling it up or down. This can be useful for simulating variations in the loudness of the audio.\n",
    "\n",
    "- Add noise : This technique adds noise to the audio signal to simulate different noise conditions.\n",
    "\n",
    "- Time shifting: This technique changes the position of the audio signal in time by shifting it forwards or backwards. This can be useful for simulating variations in the timing of the audio.\n",
    "\n",
    "- Echo: This technique adds a delayed copy of the audio signal to simulate an echo effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, *, mean=0, std=0.1):\n",
    "    noise = np.random.normal(mean, std, len(data))\n",
    "    audio_noisy = data + noise\n",
    "    return audio_noisy\n",
    "    \n",
    "def pitch_shifting(data):\n",
    "    sr  = 16000\n",
    "    bins_per_octave = 12\n",
    "    pitch_pm = 2\n",
    "    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n",
    "    data = librosa.effects.pitch_shift(y=data.astype('float64'),  sr=sr, n_steps=pitch_change, \n",
    "                                          bins_per_octave=bins_per_octave)\n",
    "    return data\n",
    "\n",
    "def random_shift(data):\n",
    "    timeshift_fac = 0.2 *2*(np.random.uniform()-0.5)  # up to 20% of length\n",
    "    start = int(data.shape[0] * timeshift_fac)\n",
    "    if (start > 0):\n",
    "        data = np.pad(data,(start,0),mode='constant')[0:data.shape[0]]\n",
    "    else:\n",
    "        data = np.pad(data,(0,-start),mode='constant')[0:data.shape[0]]\n",
    "    return data\n",
    "\n",
    "def volume_scaling(data):\n",
    "    sr  = 16000\n",
    "    dyn_change = np.random.uniform(low=1.5,high=2.5)\n",
    "    data = data * dyn_change\n",
    "    return data\n",
    "    \n",
    "def time_stretching(data, rate=1.5):\n",
    "    input_length = len(data)\n",
    "    streching = data.copy()\n",
    "    streching = librosa.effects.time_stretch(streching, rate)\n",
    "    \n",
    "    if len(streching) > input_length:\n",
    "        streching = streching[:input_length]\n",
    "    else:\n",
    "        streching = np.pad(data, (0, max(0, input_length - len(streching))), \"constant\")\n",
    "    return streching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('data/background/background_00.wav')\n",
    "Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ee1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(add_noise(y, std=0.01), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e55479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(pitch_shifting(y), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe716a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lectures",
   "language": "python",
   "name": "python_lectures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
